# -*- coding: utf-8 -*-
"""Neuronet_images.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LSHrNwb9Hq_iTA37CfYRVEtdYb4Oc25t
"""

import numpy as np
from keras.datasets import fashion_mnist #датасет (на просто mnist работает очень точно)
from keras.engine.sequential import Sequential #последовательная сеть
from keras.layers.core import Dense #полносвязная сеть
from keras import utils #утилиты
import matplotlib.pyplot as plt
import random as rnd

#загружаем данные из датасета
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() #x_train - изображения для обучения, y_train - правильные ответы, x_test ,y_test - набор данных для тестирования
#определим названия классов
classes = ['футболка','брюки','свитер','платье','пальто','туфли','рубашка','кроссовки','сумка','ботинки']
#посмотрим, что у нас дано в качестве примера
plt.figure(figsize=(10,10))
for i in range(100,150):
    plt.subplot(5,10,i-100+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
    plt.xlabel(classes[y_train[i]])
plt.show()
#преобразуем двумерные изображения в одномерные данные для входа в нейросеть
x_train = x_train.reshape(60000, 784) #60000 изображений, 784 пикселя в каждом - получаем двумерный массив
x_test = x_test.reshape(10000, 784) #для тестирования 10000 изображений
#нормализуем
x_train = x_train / 255 #делим интенсивность каждого пикселя на 255, то есть теперь эти данные в диапозоне от 0 до 1
x_test = x_test / 255
#правильные ответы - номер соответствующего класса от 0 до 10, нейросеть выдает 10 значений по количеству нейронов, которые соответствуют вероятности того, что объект принадлежит данному классу
y_train = utils.to_categorical(y_train, 10) #преобразуем метки выхода в категории, пример ниже (10 классов)
y_test = utils.to_categorical(y_test, 10)

#теперь создаем саму нейросеть(модель)
model = Sequential()

#добавляем слои(2 слоя, входной на 800 и выходной на 10 нейронов)
model.add(Dense(800, input_dim = 784, activation = "relu"))#800 - количество нейронов, input_dim - количество входов в каждый нейрон(по количеству пикселей в изображении), activation - функция активации, в данном случае - ReLu - полулинейная функция (при x < 0 , y = 0, при x > 0, y = x)
model.add(Dense(400, activation = "relu"))#добавление дополнительных слоев в разы ускорило скорость обучения, но скорее начало наступать и переобучение и поэтому приходится использовать меньше эпох
model.add(Dense(200, activation = "relu"))
model.add(Dense(10, activation = "softmax"))#здесь функция активации softmax - мягкий максимум(нормализованная экспоненциальная функция, она позволяет получить суммарное значение всех нейронов на выходе равным 1, то есть трактовать выход нейронов как вероятность)

#компилируем модель
model.compile(loss = "categorical_crossentropy", optimizer = "SGD", metrics = ["accuracy"])
#loss - функция ошибки(в данном случае, категориальная перекрестная энтропия - хорошо подходит для задач классификации когда классов больше чем 2), optimiser - оптимизатор(SGD = стохастический градиентный спуск), metrics - метрика качества обучения сети(доля правильных ответов)
print(model.summary()) # печатаем параметры сети

#обучение
model.fit(x_train, y_train, batch_size = 200, epochs = 30, validation_split = 0.2, verbose = 1)#validation split - часть наборя для обучения используется для проверки
#x_train - вход, y_train - выход, batch_size - размер минивыборки для метода градиентного спуска(то есть мы берем 200 изображений, рассчитываем функцию ошибки, потом расчитываем градиент и изменяем веса), epochs - сколько раз мы обучаем нейросеть на одном и том же наборе данных, verbose - позволяет печатать прогресс обучения нейросети
#запускаем процесс оценки качества на тестовой выборке
scores = model.evaluate(x_test, y_test, verbose=1)
print("Доля верных ответов на тестовых данных, в процентах: ", round(scores[1] * 100, 4))
#запускаем сеть на тестовых данных(10 случайных)
for i in range(1, 11):
  n_rec = rnd.randint(0, 10000)#элемент массива x_test
  print("Выбран элемент массива x_test под номером: ", n_rec)
  plt.imshow(x_test[n_rec].reshape(28, 28), cmap=plt.cm.binary)
  #показываем наше изображение для теста
  label = np.argmax(y_test[n_rec], axis = 0)#здесь мы находим максимальный элемент массива y_test и выводим его индекс, соответствующий номеру класса
  plt.xlabel(classes[label])
  plt.show()
  testone = x_test[n_rec]
  testone = np.expand_dims(testone, axis = 0)#нормализуем изображение
  prediction = model.predict(testone)
  prediction = np.argmax(prediction[0])#здесь мы находим максимальный элемент массива prediction и выводим его индекс, соответствующий номеру класса
  print("Ответ нейросети: ", classes[prediction])
  label = np.argmax(y_test[n_rec], axis = 0)
  print("Правильный ответ: ", classes[label])
#переобучение наступает, когда val_acc начинает падать, это значит, что нейросеть учится распознавать только элементы конкретной выборки, а не общие особенности
#все параметры выбраны оптимальными экспериментально

